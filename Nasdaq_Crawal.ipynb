{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SYM            PRICE          UP/DOWN   PERCENTS        netChange        LASTCLOSE\n",
      "       =========        =========        =========  =========        =========        =========\n",
      "            AAPL          $131.97               up      0.77%            +1.01          $130.96\n",
      "            TSLA          $661.77               up      2.44%           +15.79          $645.98\n",
      "            VLDR           $24.68             down      6.87%            -1.82           $26.50\n",
      "            LAZR           $33.06             down      6.13%            -2.16           $35.22\n",
      "            MRNA          $123.39             down      5.33%            -6.95          $130.34\n",
      "             PFE           $37.27             down      0.45%            -0.17           $37.44\n",
      "             TSM          $105.97               up      2.13%            +2.21          $103.76\n",
      "             UMC            $8.29               up       1.1%            +0.09            $8.20\n",
      "             AMD           $91.81               up      0.28%            +0.26           $91.55\n",
      "            INTC           $47.07               up      1.07%            +0.50           $46.57\n",
      "              FB          $267.40             down      0.26%            -0.71          $268.11\n",
      "            GOOG         $1738.85               up      0.37%            +6.47         $1732.38\n",
      "            SQNS            $6.29             down      0.79%            -0.05            $6.34\n",
      "            ORBC            $6.91               up      0.88%            +0.06            $6.85\n",
      "           NEXCF          $5.0944               up      0.48%          +0.0244            $5.07\n",
      "            DKNG           $52.11             down       3.3%            -1.78           $53.89\n",
      "            GOCO           $14.10               up      1.29%            +0.18           $13.92\n",
      "Done. 更新時間:2020-12-27:10:51:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-14c8b595442d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_crawler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.6/sched.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, blocking)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mdelayfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from datetime import date\n",
    "import requests\n",
    "import datetime\n",
    "import sched\n",
    "import time as tm\n",
    "from time import gmtime, strftime                \n",
    "                \n",
    "s = sched.scheduler(tm.time, tm.sleep)\n",
    "node_id = \"NODE_000\"\n",
    "\n",
    "# This example requires the requests library be installed.  You can learn more\n",
    "# about the Requests library here: http://docs.python-requests.org/en/latest/\n",
    "from requests import get\n",
    "my_ip = get('https://api.ipify.org').text\n",
    "print('My public IP address is: ProcessNode {} IP {}'.format(node_id, my_ip))\n",
    "\n",
    "class CrawlerController(object):\n",
    "    '''Split targets into several Crawler, avoid request url too long'''\n",
    "\n",
    "    def __init__(self, targets, max_stock_per_crawler=1):\n",
    "        self.crawlers = []\n",
    "        print ( '{0:>16s} {1:>16s} {2:>16s} {3:>10s} {4:>16s} {5:>16s}'.format(\"SYM\",      \"PRICE\"    ,\"UP/DOWN\"  ,\"PERCENTS\",\"netChange\",\"LASTCLOSE\"))\n",
    "        print ( '{0:>16s} {1:>16s} {2:>16s} {3:>10s} {4:>16s} {5:>16s}'.format(\"=========\",\"=========\",\"=========\",\"=========\",\"=========\",\"=========\"))\n",
    "        for index in range(0, len(targets), max_stock_per_crawler):\n",
    "            crawler = Crawler(targets[index:index + max_stock_per_crawler])\n",
    "            self.crawlers.append(crawler)\n",
    "\n",
    "    def run(self):\n",
    "        data = []\n",
    "        for crawler in self.crawlers:\n",
    "            data.extend(crawler.get_data())\n",
    "        return data\n",
    "\n",
    "class Crawler(object):\n",
    "    '''Request to Market Information System'''\n",
    "    def __init__(self, targets):\n",
    "        endpoint = 'https://api.nasdaq.com/api/quote/'\n",
    "        # Add 1000 seconds for prevent time inaccuracy\n",
    "        # timestamp = int(time.time() * 1000 + 1000000)\n",
    "        ## channels = '|'.join('tse_{}.tw'.format(target) for target in targets)\n",
    "        ##  please mark tse_|otc_ markets in stocknumber.csv \n",
    "        channels = '|'.join('{}'.format(target) for target in targets)\n",
    "        self.query_url = '{}{}/chart?assetclass=stocks'.format(endpoint, channels)\n",
    "        \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            # Get original page to get session\n",
    "            # req = requests.session()\n",
    "            # req.get('http://mis.twse.com.tw/stock/index.jsp',\n",
    "            #         headers={'Accept-Language': 'zh-TW'})\n",
    "\n",
    "            \n",
    "            headers = {\n",
    "                        'authority': 'www.nasdaq.com',\n",
    "                        'upgrade-insecure-requests': '1',\n",
    "                        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36',\n",
    "                        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "                        'sec-fetch-site': 'none',\n",
    "                        'sec-fetch-mode': 'navigate',\n",
    "                        'sec-fetch-user': '?1',\n",
    "                        'sec-fetch-dest': 'document',\n",
    "                        'accept-language': 'en-US,en;q=0.9',\n",
    "                    }\n",
    "\n",
    "            # page_response = requests.get(h_url, timeout=30, allow_redirects=True, headers=headers)\n",
    "            \n",
    "            response = requests.get(self.query_url, timeout=30, allow_redirects=True, headers=headers)\n",
    "            content = json.loads(response.text)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"GetData Fatal: \",err)\n",
    "            data = []\n",
    "            \n",
    "        else:\n",
    "            tdata = content['data']\n",
    "        \n",
    "            print ( '{0:>16s} {1:>16s} {2:>16s} {3:>10s} {4:>16s} {5:>16s}'.format(tdata[\"symbol\"], \\\n",
    "                                                                                   tdata[\"lastSalePrice\"] , \\\n",
    "                                                                                   tdata[\"deltaIndicator\"], \\\n",
    "                                                                                   tdata[\"percentageChange\"], \\\n",
    "                                                                                   \n",
    "                                                                                   tdata[\"netChange\"], \\\n",
    "                                                                                   tdata[\"previousClose\"]) )\n",
    "            ddata = [{\n",
    "                \"symbol\" : tdata[\"symbol\"], \n",
    "                \"company\" : tdata[\"company\"],\n",
    "                \"lastSalePrice\" : tdata[\"lastSalePrice\"],   \n",
    "                \"previousClose\" : tdata[\"previousClose\"],\n",
    "                \"netChange\" : tdata[\"netChange\"],\n",
    "                \"percentageChange\" : tdata[\"percentageChange\"],    \n",
    "                \"deltaIndicator\" : tdata[\"deltaIndicator\"],             \n",
    "            }]\n",
    "            data = ddata\n",
    "\n",
    "        return data\n",
    "\n",
    "class Recorder(object):\n",
    "    '''Record data to csv'''\n",
    "    def __init__(self, path='data'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.mkdir(path)                # if no data path created then create it \n",
    "        self.folder_path = '{}/{}'.format(path, date.today().strftime('%Y%m%d'))\n",
    "        if not os.path.isdir(self.folder_path):\n",
    "            os.mkdir(self.folder_path)\n",
    "\n",
    "    def record_to_csv(self, data):\n",
    "        UTC_Time = datetime.datetime.utcnow().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "   \n",
    "        for row in data:\n",
    "            try:\n",
    "                file_path = '{}/{}.csv'.format(self.folder_path, row['symbol'])\n",
    "                 \n",
    "                with open(file_path, 'a') as output_file:\n",
    "                    writer = csv.writer(output_file, delimiter=',')\n",
    "                    writer.writerow ([\n",
    "                        UTC_Time, \n",
    "                        row['symbol'],           # 資料時間\n",
    "                        row['company'],          # 資料時間\n",
    "                        row['lastSalePrice'],    # 資料時間\n",
    "                        row['percentageChange'],    # 資料時間\n",
    "                        row['deltaIndicator'],    # 資料時間\n",
    "                        row['netChange'], # \n",
    "                        row['previousClose'], \n",
    "                        node_id, \n",
    "                        my_ip,                   # my public ip address\n",
    "                    ])\n",
    "\n",
    "            except Exception as err:\n",
    "                print(\"Record CSV error: \", err)\n",
    "\n",
    "def main_crawler ():   \n",
    "    \n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    targets = [_.strip() for _ in open('stocknumber.csv', 'r')]\n",
    "\n",
    "    UTC_Time = datetime.datetime.utcnow().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "    time = datetime.datetime.now()  \n",
    "    timestr = str(time.date())+':'+str(time.hour)+\":\"+str(time.minute)+\":\"+str(time.second)\n",
    "    print(\"Nasdaq trading time: Pre-Market time start from 4:00ET (17PM TPI) to 7:30ET \")\n",
    "    print(\"                     Normal trading time start from 9:30ET to 16:00ET\")\n",
    "    print(\"開始更新時間:\" + timestr, \" UTC Time: \", UTC_Time)\n",
    "\n",
    "    start_time = datetime.datetime.strptime(str(time.date())+'17:00', '%Y-%m-%d%H:%M')\n",
    "    end_time =  datetime.datetime.strptime(str(time.date())+'5:00', '%Y-%m-%d%H:%M')\n",
    "    \n",
    "    # tm.sleep (3) # 避免證交所伺服器鎖 IP，可能為都是網頁伺服器的rate limiting 在作祟。\n",
    "    # 判斷爬蟲終止條件\n",
    "    sleeptimer = 0.5\n",
    "    \n",
    "    if ((start_time < end_time) and (time >= start_time and time <= end_time)) or \\\n",
    "       ((start_time > end_time) and not (time <= start_time and time >= end_time)) :  # 處理跨日情況\n",
    "        tm.sleep (sleeptimer)\n",
    "        try: \n",
    "            clear_output(wait=True)\n",
    "            controller = CrawlerController(targets)\n",
    "            data = controller.run()\n",
    "\n",
    "            recorder = Recorder()\n",
    "            recorder.record_to_csv(data)\n",
    "        except Exception as err:\n",
    "            msg =  err\n",
    "        else:\n",
    "            msg = \"更新時間:\" + str(time.date())+':'+str(time.hour)+\":\"+str(time.minute)+\":\"+str(time.second)\n",
    "        finally:\n",
    "            # print(\"更新時間:\" + str(time.date())+':'+str(time.hour)+\":\"+str(time.minute)+\":\"+str(time.second))\n",
    "            print(\"Done.\", msg)\n",
    "            s.enter(5, 0, main_crawler, argument=())\n",
    "            tm.sleep (sleeptimer)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # trade time off \n",
    "        '''\n",
    "        if TIMEUP == 1: \n",
    "            print ('非營業時間，不提供連續資料。')\n",
    "            print ('繼續等待交易時間。。。')\n",
    "            TIMEUP = 0\n",
    "        '''\n",
    "        s.enter(100, 0, main_crawler, argument=())        \n",
    "                \n",
    "def main():\n",
    "    targets = [_.strip() for _ in open('stocknumber.csv', 'r')]\n",
    "\n",
    "    controller = CrawlerController(targets)\n",
    "    data = controller.run()\n",
    "\n",
    "    recorder = Recorder()\n",
    "    recorder.record_to_csv(data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    \n",
    "    s.enter(1, 0, main_crawler, argument=())\n",
    "    s.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 0;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-20b0f00b95c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m '''\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"symbol       \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"symbol\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m                     return complexjson.loads(\n\u001b[0;32m--> 884\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                     )\n\u001b[1;32m    886\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, **kw)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0mparse_constant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             and not use_decimal and not kw):\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_PY3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mord0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xef\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\xef\\xbb\\xbf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "h_url= 'https://www.nasdaq.com/api/v1/historical/vldr/stocks/2020-06-30/2020-12-11'\n",
    "# h_url= 'https://api.nasdaq.com/api/quote/aapl/chart?assetclass=stocks'\n",
    "headers = {\n",
    "    'authority': 'www.nasdaq.com',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "}\n",
    "\n",
    "page_response = requests.get(h_url, timeout=30, allow_redirects=True, headers=headers)\n",
    "\n",
    "# print (page_response.content) \n",
    "\n",
    "with open(\"dump.txt\", \"w\") as out:\n",
    "    out.write(str(page_response.content))\n",
    "        \n",
    "import json\n",
    "data = page_response.json()\n",
    "'''\n",
    "print (\"symbol       \", data[\"data\"][\"symbol\"]) \n",
    "print (\"lastSalePrice\", data[\"data\"][\"lastSalePrice\"]) \n",
    "print (\"percentage   \", data[\"data\"][\"percentageChange\"]) \n",
    "print (\"deltaIndicat \", data[\"data\"][\"deltaIndicator\"]) \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "from urllib.request import urlopen\n",
    "# from urllib import urlopen\n",
    "\n",
    "path = 'http://ws.nasdaqdod.com/v1/NASDAQAnalytics.asmx/ListMarketCenters?%20HTTP/1.1'\n",
    "xml = objectify.parse(urlopen(path))\n",
    "\n",
    "columns = ['Outcome', 'Identity', 'Identity', 'Symbol', 'Code', 'Name']\n",
    "root = xml.getroot()\n",
    "print (columns)\n",
    "print (len(root.getchildren()))\n",
    "\n",
    "l = [ line.getchildren() for line in root.getchildren() ]\n",
    "lp = [ line.getchildren() for line in root.getchildren() ]\n",
    "\n",
    "df = pd.DataFrame(data=l, columns=columns)\n",
    "\n",
    "print (lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historical Stock Prices\n",
    "Reference: http://www.nasdaq.com/symbol/pih/historical\n",
    "\n",
    "Historical Stock Prices CSV file download link sample \n",
    "'https://www.nasdaq.com/api/v1/historical/vldr/stocks/2020-06-30/2020-12-11'\n",
    "\n",
    "Realtime stock price \n",
    "'https://api.nasdaq.com/api/quote/aapl/chart?assetclass=stocks'\n",
    "\n",
    "\n",
    "Market time\n",
    "Pre-Market time\n",
    "Real trade time    ET 4:15 ~ 7:30 AM\n",
    "After Market       ET16:15 ~ 15:30 PM(Following day)\n",
    "Data is delayed at least 15 minutes. Nasdaq.com will report pre-market and after hours trades.\n",
    "Pre-Market trade data will be posted from 4:15 a.m. ET to 7:30 a.m. ET of the following day.\n",
    "After Hours trades will be posted from 4:15 p.m. ET to 3:30 p.m. ET of the following day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://www.nasdaq.com/symbol/pfe/historical'\n",
    "print ('making request to: {0}'.format(url))\n",
    "\n",
    "headers = {\n",
    "    'authority': 'www.nasdaq.com',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "}\n",
    "\n",
    "\n",
    "content = urlopen(url).read()\n",
    "#soup = BeautifulSoup(content)\n",
    "#titles = soup.select('div#quotes_content_left_pnlAJAX table tbody tr td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "tf_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
